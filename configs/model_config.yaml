# Model Configuration
# Defines LLM model settings and routing rules

# Default model settings
defaults:
  model: gpt-4o
  temperature: 0.7
  max_tokens: 4096
  top_p: 1.0
  frequency_penalty: 0.0
  presence_penalty: 0.0

# Model definitions
models:
  # OpenAI Models
  gpt-4o:
    provider: openai
    model_id: gpt-4o
    context_window: 128000
    max_output_tokens: 16384
    capabilities:
      - chat
      - function_calling
      - vision
    cost_per_1k_input: 0.005
    cost_per_1k_output: 0.015
    
  gpt-4o-mini:
    provider: openai
    model_id: gpt-4o-mini
    context_window: 128000
    max_output_tokens: 16384
    capabilities:
      - chat
      - function_calling
      - vision
    cost_per_1k_input: 0.00015
    cost_per_1k_output: 0.0006
    
  gpt-4-turbo:
    provider: openai
    model_id: gpt-4-turbo
    context_window: 128000
    max_output_tokens: 4096
    capabilities:
      - chat
      - function_calling
      - vision
    cost_per_1k_input: 0.01
    cost_per_1k_output: 0.03

  # Anthropic Models
  claude-3-5-sonnet:
    provider: anthropic
    model_id: claude-3-5-sonnet-20241022
    context_window: 200000
    max_output_tokens: 8192
    capabilities:
      - chat
      - function_calling
      - vision
    cost_per_1k_input: 0.003
    cost_per_1k_output: 0.015
    
  claude-3-5-haiku:
    provider: anthropic
    model_id: claude-3-5-haiku-20241022
    context_window: 200000
    max_output_tokens: 8192
    capabilities:
      - chat
      - function_calling
    cost_per_1k_input: 0.001
    cost_per_1k_output: 0.005

  claude-3-opus:
    provider: anthropic
    model_id: claude-3-opus-20240229
    context_window: 200000
    max_output_tokens: 4096
    capabilities:
      - chat
      - function_calling
      - vision
    cost_per_1k_input: 0.015
    cost_per_1k_output: 0.075

  # Offline/Local Models
  stub:
    provider: stub
    model_id: stub
    context_window: 8192
    max_output_tokens: 2048
    capabilities:
      - chat
    description: "Stub model for offline testing"

# Routing rules
routing:
  # Default routing
  default: gpt-4o
  
  # Task-based routing
  tasks:
    simple_chat:
      preferred: gpt-4o-mini
      fallback: claude-3-5-haiku
      
    complex_reasoning:
      preferred: claude-3-opus
      fallback: gpt-4-turbo
      
    code_generation:
      preferred: claude-3-5-sonnet
      fallback: gpt-4o
      
    vision:
      preferred: gpt-4o
      fallback: claude-3-5-sonnet
      requires: vision
      
  # Cost-based routing
  cost_tiers:
    economy:
      - gpt-4o-mini
      - claude-3-5-haiku
      - stub
    standard:
      - gpt-4o
      - claude-3-5-sonnet
    premium:
      - claude-3-opus
      - gpt-4-turbo

# Rate limits
rate_limits:
  openai:
    requests_per_minute: 500
    tokens_per_minute: 30000
  anthropic:
    requests_per_minute: 300
    tokens_per_minute: 40000

# Retry configuration
retry:
  max_retries: 3
  initial_delay: 1.0
  max_delay: 60.0
  exponential_base: 2.0
  retryable_errors:
    - rate_limit
    - server_error
    - timeout

# Offline mode
offline:
  enabled: false
  model: stub
  cache_responses: true
